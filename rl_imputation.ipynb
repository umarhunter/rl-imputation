{
 "cells": [
  {
   "cell_type": "code",
   "id": "369c60a6-c375-4c0c-9a4e-824e579d54b4",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-08-07T00:08:31.121429Z",
     "start_time": "2024-08-07T00:08:30.788654Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "f052c8aa-3a83-4b8a-b11c-a815279a7c6c",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-08-07T00:08:32.128251Z",
     "start_time": "2024-08-07T00:08:32.126112Z"
    }
   },
   "source": [
    "def introduce_missingness(df, missing_rate):\n",
    "    \"\"\"Introduce missing values randomly into the dataframe at the specified rate.\"\"\"\n",
    "    df_with_missing = df.copy()\n",
    "\n",
    "    # Total number of elements in the dataframe\n",
    "    total_elements = df_with_missing.size\n",
    "    \n",
    "    # Number of elements to be set as NaN\n",
    "    num_missing = int(missing_rate * total_elements)\n",
    "    \n",
    "    # Get random indices\n",
    "    missing_indices = np.random.choice(total_elements, num_missing, replace=False)\n",
    "    \n",
    "    # Convert the flat indices to multi-dimensional indices\n",
    "    multi_dim_indices = np.unravel_index(missing_indices, df_with_missing.shape)\n",
    "    \n",
    "    # Assign NaN to the chosen indices\n",
    "    df_with_missing.values[multi_dim_indices] = np.nan\n",
    "    return df_with_missing"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "ba0c5cec-6a1b-42b4-a4e4-49ef30dca1c9",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-08-07T00:08:32.986658Z",
     "start_time": "2024-08-07T00:08:32.974520Z"
    }
   },
   "source": [
    "data_path = \"data/toy_dataset.csv\"\n",
    "df = pd.read_csv(data_path, na_values='?')"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "527a8e7e-b161-41ba-8d5b-fed3a19037e6",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-08-07T00:08:37.040961Z",
     "start_time": "2024-08-07T00:08:37.029555Z"
    }
   },
   "source": [
    "missingness_rates = np.linspace(0.1, 1, 10)\n",
    "\n",
    "for miss_rate in missingness_rates:\n",
    "    df_with_missing = introduce_missingness(df, miss_rate)\n",
    "    print(f'Missing Rate: {miss_rate}')\n",
    "    print(df_with_missing.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Rate: 0.1\n",
      "   Col 1  Col 2  Col 3  Col 4\n",
      "0   0.17    NaN   0.57   1.00\n",
      "1   0.50   0.53   0.00    NaN\n",
      "2   0.83   0.00   0.57   0.33\n",
      "3   0.17   0.39   0.87   0.50\n",
      "4   1.00    NaN   0.14   0.67\n",
      "Missing Rate: 0.2\n",
      "   Col 1  Col 2  Col 3  Col 4\n",
      "0   0.17   0.26   0.57   1.00\n",
      "1   0.50   0.53   0.00   0.83\n",
      "2   0.83   0.00    NaN   0.33\n",
      "3   0.17   0.39   0.87   0.50\n",
      "4   1.00   0.53   0.14   0.67\n",
      "Missing Rate: 0.30000000000000004\n",
      "   Col 1  Col 2  Col 3  Col 4\n",
      "0   0.17   0.26    NaN   1.00\n",
      "1   0.50   0.53    NaN   0.83\n",
      "2   0.83    NaN   0.57    NaN\n",
      "3   0.17   0.39   0.87    NaN\n",
      "4   1.00    NaN    NaN   0.67\n",
      "Missing Rate: 0.4\n",
      "   Col 1  Col 2  Col 3  Col 4\n",
      "0   0.17   0.26   0.57    NaN\n",
      "1   0.50   0.53   0.00   0.83\n",
      "2    NaN    NaN    NaN    NaN\n",
      "3   0.17   0.39   0.87   0.50\n",
      "4   1.00   0.53   0.14   0.67\n",
      "Missing Rate: 0.5\n",
      "   Col 1  Col 2  Col 3  Col 4\n",
      "0    NaN    NaN   0.57   1.00\n",
      "1   0.50    NaN   0.00   0.83\n",
      "2    NaN    NaN   0.57   0.33\n",
      "3   0.17    NaN   0.87    NaN\n",
      "4   1.00    NaN    NaN    NaN\n",
      "Missing Rate: 0.6\n",
      "   Col 1  Col 2  Col 3  Col 4\n",
      "0    NaN   0.26    NaN    NaN\n",
      "1    NaN   0.53    NaN   0.83\n",
      "2    NaN   0.00    NaN   0.33\n",
      "3   0.17    NaN   0.87    NaN\n",
      "4    NaN   0.53   0.14    NaN\n",
      "Missing Rate: 0.7000000000000001\n",
      "   Col 1  Col 2  Col 3  Col 4\n",
      "0    NaN   0.26    NaN    NaN\n",
      "1    NaN    NaN    NaN    NaN\n",
      "2   0.83   0.00    NaN    NaN\n",
      "3   0.17   0.39    NaN    0.5\n",
      "4    NaN    NaN    NaN    NaN\n",
      "Missing Rate: 0.8\n",
      "   Col 1  Col 2  Col 3  Col 4\n",
      "0    NaN    NaN   0.57    NaN\n",
      "1    NaN    NaN    NaN    NaN\n",
      "2    NaN    0.0   0.57    NaN\n",
      "3    NaN    NaN    NaN    NaN\n",
      "4    NaN    NaN   0.14   0.67\n",
      "Missing Rate: 0.9\n",
      "   Col 1  Col 2  Col 3  Col 4\n",
      "0    NaN    NaN    NaN    NaN\n",
      "1    NaN    NaN    0.0    NaN\n",
      "2   0.83    NaN    NaN    NaN\n",
      "3    NaN    NaN    NaN    NaN\n",
      "4    NaN    NaN    NaN    NaN\n",
      "Missing Rate: 1.0\n",
      "   Col 1  Col 2  Col 3  Col 4\n",
      "0    NaN    NaN    NaN    NaN\n",
      "1    NaN    NaN    NaN    NaN\n",
      "2    NaN    NaN    NaN    NaN\n",
      "3    NaN    NaN    NaN    NaN\n",
      "4    NaN    NaN    NaN    NaN\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "1d1c32aa-d97e-496a-a2c8-6c9c4babb38d",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-08-07T00:08:37.568375Z",
     "start_time": "2024-08-07T00:08:37.563543Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class ImputationEnvironment:\n",
    "    def __init__(self, incomplete_data, complete_data):\n",
    "        self.incomplete_data = incomplete_data\n",
    "        self.complete_data = complete_data\n",
    "        self.state = incomplete_data.copy()\n",
    "        self.missing_indices = np.argwhere(pd.isna(incomplete_data.values))\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = self.incomplete_data.copy()\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action, position):\n",
    "        row, col = position\n",
    "        self.state.iat[row, col] = action\n",
    "\n",
    "        reward = -abs(self.complete_data.iat[row, col] - action)\n",
    "        done = not pd.isna(self.state.values).any()\n",
    "        return self.state, reward, done\n",
    "\n",
    "    def get_possible_actions(self, col):\n",
    "        # Ensure col is an integer if it's a numpy int\n",
    "        if isinstance(col, np.int64):\n",
    "            col = int(col)\n",
    "\n",
    "        if isinstance(col, int):\n",
    "            if 0 <= col < len(self.complete_data.columns):\n",
    "                col_name = self.complete_data.columns[col]\n",
    "            else:\n",
    "                raise KeyError(f\"Column index {col} out of range\")\n",
    "        elif isinstance(col, str):\n",
    "            if col in self.complete_data.columns:\n",
    "                col_name = col\n",
    "            else:\n",
    "                raise KeyError(f\"Column name '{col}' not found in DataFrame\")\n",
    "        else:\n",
    "            raise TypeError(\"Column must be either an integer index or a string column name\")\n",
    "\n",
    "        return self.complete_data[col_name].dropna().unique()\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "    def choose_action(self, state, position):\n",
    "        state_key = (tuple(state.values.flatten()), tuple(position))\n",
    "        \n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(self.env.get_possible_actions(position[1]))\n",
    "        else:\n",
    "            col = position[1]\n",
    "            actions = self.env.get_possible_actions(col)\n",
    "            q_values = {a: self.q_table[state_key][a] for a in actions}\n",
    "            return max(q_values, key=q_values.get)\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, position):\n",
    "        state_key = (tuple(state.values.flatten()), tuple(position))\n",
    "        next_state_key = (tuple(next_state.values.flatten()), tuple(position))\n",
    "\n",
    "        q_predict = self.q_table[state_key][action]\n",
    "        q_target = reward + self.gamma * max(self.q_table[next_state_key].values(), default=0)\n",
    "        self.q_table[state_key][action] += self.alpha * (q_target - q_predict)\n",
    "\n",
    "    def train(self, episodes=1000):\n",
    "        for _ in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                position = random.choice(self.env.missing_indices)\n",
    "                action = self.choose_action(state, position)\n",
    "                next_state, reward, done = self.env.step(action, position)\n",
    "                self.learn(state, action, reward, next_state, position)\n",
    "                state = next_state"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "00c43aff-591c-464a-bd53-614be4c594d2",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-08-07T00:08:44.240621Z",
     "start_time": "2024-08-07T00:08:44.234117Z"
    }
   },
   "source": [
    "# Load data and train the agent\n",
    "incomplete_data_path = 'data/toy_dataset_missing.csv'\n",
    "complete_data_path = 'data/toy_dataset.csv'\n",
    "\n",
    "incomplete_data = pd.read_csv(incomplete_data_path)\n",
    "complete_data = pd.read_csv(complete_data_path)\n",
    "\n",
    "incomplete_data.replace(\"?\", np.nan, inplace=True)\n",
    "complete_data.replace(\"?\", np.nan, inplace=True) # we shouldn't really have missing data here but wtvr"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "622f4e06-4ef5-4d97-8b39-e5f867afc944",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-08-07T00:08:47.215014Z",
     "start_time": "2024-08-07T00:08:47.212281Z"
    }
   },
   "source": [
    "print(complete_data.columns)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Col 1', 'Col 2', 'Col 3', 'Col 4'], dtype='object')\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "70e0ce27-5655-4db5-b82c-aa00ef595c06",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-08-07T00:14:48.600315Z",
     "start_time": "2024-08-07T00:08:48.669580Z"
    }
   },
   "source": [
    "#Optional: Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "incomplete_data = pd.DataFrame(scaler.fit_transform(incomplete_data), columns=incomplete_data.columns)\n",
    "complete_data = pd.DataFrame(scaler.transform(complete_data), columns=complete_data.columns)\n",
    "\n",
    "env = ImputationEnvironment(incomplete_data, complete_data)\n",
    "agent = QLearningAgent(env)\n",
    "\n",
    "agent.train(episodes=300000)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ca810de-6581-4cd1-8e04-d356992be29e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Example calls\n",
    "# col_index = 3  # Example of integer index\n",
    "# col_name = 'Col 4'  # Example of column name\n",
    "\n",
    "# actions_by_index = env.get_possible_actions(col_index)\n",
    "# actions_by_name = env.get_possible_actions(col_name)\n",
    "# print(actions_by_index)\n",
    "# print(actions_by_name)"
   ]
  },
  {
   "cell_type": "code",
   "id": "8581e3e1-e126-4da9-aa66-57723aaf792d",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-08-07T00:21:10.852907Z",
     "start_time": "2024-08-07T00:21:10.840133Z"
    }
   },
   "source": [
    "# Imputed data\n",
    "imputed_data = env.state\n",
    "print(imputed_data)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Col 1  Col 2  Col 3  Col 4\n",
      "0   0.17   0.26   0.57   1.00\n",
      "1   0.50   0.53   0.00   0.83\n",
      "2   0.83   0.00   0.57   1.00\n",
      "3   0.17   0.26   0.87   0.50\n",
      "4   1.00   0.53   0.57   0.67\n",
      "5   0.17   0.84   0.86   0.00\n",
      "6   0.85   0.26   0.17   0.83\n",
      "7   0.03   1.00   0.71   1.00\n",
      "8   0.17   0.13   0.86   0.83\n",
      "9   0.00   0.26   1.00   0.70\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "7cafe688-0b74-475e-8ce4-6649aa306043",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T00:32:03.101307Z",
     "start_time": "2024-08-07T00:30:58.104711Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Environment remains the same as in your original implementation\n",
    "\n",
    "class ImputationEnvironment:\n",
    "    def __init__(self, incomplete_data, complete_data):\n",
    "        self.incomplete_data = incomplete_data\n",
    "        self.complete_data = complete_data\n",
    "        self.state = incomplete_data.copy()\n",
    "        self.missing_indices = np.argwhere(pd.isna(incomplete_data.values))\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.incomplete_data.copy()\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action, position):\n",
    "        row, col = position\n",
    "        self.state.iat[row, col] = action\n",
    "\n",
    "        reward = -abs(self.complete_data.iat[row, col] - action)\n",
    "        done = not pd.isna(self.state.values).any()\n",
    "        return self.state, reward, done\n",
    "\n",
    "    def get_possible_actions(self, col):\n",
    "        col = int(col)\n",
    "        if 0 <= col < len(self.complete_data.columns):\n",
    "            col_name = self.complete_data.columns[col]\n",
    "        else:\n",
    "            raise KeyError(f\"Column index {col} out of range\")\n",
    "\n",
    "        return self.complete_data[col_name].dropna().unique()\n",
    "\n",
    "# Deep Q-Learning Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, hidden_size=24, gamma=0.95, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995, learning_rate=0.001, batch_size=32, memory_size=2000):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.gamma = gamma    # discount rate\n",
    "        self.epsilon = epsilon  # exploration rate\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.model = self._build_model(hidden_size)\n",
    "\n",
    "    def _build_model(self, hidden_size):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(self.state_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, self.action_size)\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        state = torch.FloatTensor(state)\n",
    "        act_values = self.model(state)\n",
    "        return torch.argmax(act_values[0]).item()\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                next_state = torch.FloatTensor(next_state)\n",
    "                target = (reward + self.gamma *\n",
    "                          torch.max(self.model(next_state)[0]).item())\n",
    "            target_f = self.model(torch.FloatTensor(state))\n",
    "            target_f[0][action] = target\n",
    "            target_f = target_f.detach()  # Detach target from computation graph\n",
    "            state = torch.FloatTensor(state)\n",
    "            # Compute loss and optimize the model\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(target_f, self.model(state))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_state_dict(torch.load(name))\n",
    "\n",
    "    def save(self, name):\n",
    "        torch.save(self.model.state_dict(), name)\n",
    "\n",
    "# Load data\n",
    "incomplete_data = pd.read_csv('data/toy_dataset_missing.csv')\n",
    "complete_data = pd.read_csv('data/toy_dataset.csv')\n",
    "\n",
    "incomplete_data.replace(\"?\", np.nan, inplace=True)\n",
    "complete_data.replace(\"?\", np.nan, inplace=True)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "incomplete_data = pd.DataFrame(scaler.fit_transform(incomplete_data), columns=incomplete_data.columns)\n",
    "complete_data = pd.DataFrame(scaler.transform(complete_data), columns=complete_data.columns)\n",
    "\n",
    "# Setup environment and agent\n",
    "env = ImputationEnvironment(incomplete_data, complete_data)\n",
    "state_size = incomplete_data.shape[1]  # assuming flat state size\n",
    "action_size = len(env.get_possible_actions(0))  # assuming all columns have similar action sizes\n",
    "agent = DQNAgent(state_size=state_size, action_size=action_size)\n",
    "\n",
    "# Train the agent\n",
    "EPISODES = 1000\n",
    "for e in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    state = state.values.flatten()\n",
    "    done = False\n",
    "    while not done:\n",
    "        position = random.choice(env.missing_indices)\n",
    "        action = agent.act(state)\n",
    "        action_value = env.get_possible_actions(position[1])[action]\n",
    "        next_state, reward, done = env.step(action_value, position)\n",
    "        next_state = next_state.values.flatten()\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "    agent.replay()\n",
    "\n",
    "# Save the trained model\n",
    "agent.save(\"dqn_model.pth\")\n",
    "\n",
    "# Print the imputed data\n",
    "imputed_data = env.state\n",
    "print(imputed_data)\n"
   ],
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 7 is out of bounds for axis 0 with size 7",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 131\u001B[0m\n\u001B[1;32m    129\u001B[0m position \u001B[38;5;241m=\u001B[39m random\u001B[38;5;241m.\u001B[39mchoice(env\u001B[38;5;241m.\u001B[39mmissing_indices)\n\u001B[1;32m    130\u001B[0m action \u001B[38;5;241m=\u001B[39m agent\u001B[38;5;241m.\u001B[39mact(state)\n\u001B[0;32m--> 131\u001B[0m action_value \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_possible_actions\u001B[49m\u001B[43m(\u001B[49m\u001B[43mposition\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[43maction\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m    132\u001B[0m next_state, reward, done \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action_value, position)\n\u001B[1;32m    133\u001B[0m next_state \u001B[38;5;241m=\u001B[39m next_state\u001B[38;5;241m.\u001B[39mvalues\u001B[38;5;241m.\u001B[39mflatten()\n",
      "\u001B[0;31mIndexError\u001B[0m: index 7 is out of bounds for axis 0 with size 7"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "960171a3ca93caa0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl-imputation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
